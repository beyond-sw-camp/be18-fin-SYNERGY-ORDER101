{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61607fd3-c642-4f56-b748-984549403a21",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "64a99ff1-dd2f-4455-b816-9912ef8f407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (2.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (1.16.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9254a264-249a-4ed6-bb5d-1c56bf035a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "591a0012-bb7e-4e07-a72f-1d074056fc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chosw\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c6c174d5-0b4f-4f67-86fb-3e0f0e1ed114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b1c50-2e66-4b08-bd82-318c41591c26",
   "metadata": {},
   "source": [
    "# 파일 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2066395a-90ca-4511-824e-3fa869fc1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: C:\\Users\\chosw\\ORDER101_114_AI\\be18-fin-SYNERGY-ORDER101\\python-server\\app\\data_pipeline\n",
      "Exists? TR: False TE: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, numpy as np, pandas as pd\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 파일 읽어오기\n",
    "candidates = [\n",
    "    Path.cwd() / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[1] / \"app\" / \"data_pipeline\",\n",
    "    Path(\"python-server/app/data_pipeline\").resolve(),\n",
    "]\n",
    "BASE = next((p for p in candidates if p.exists()), None)\n",
    "    \n",
    "TR = BASE / \"train.csv\"\n",
    "TE = BASE / \"test.csv\"\n",
    "OUT_PRED    = BASE / \"predictions.csv\"\n",
    "MODEL_PATH  = BASE / \"lightgbm_model.pkl\"\n",
    "FEATS_JSON  = BASE / \"lightgbm_features.json\"\n",
    "METRICS_CSV = BASE / \"metrics_eval.csv\"\n",
    "\n",
    "ID_KEYS = [\"warehouse_id\",\"store_id\",\"sku_id\",\"region\",\"target_date\",\"split\"]\n",
    "TARGET = \"y\"\n",
    "VAL_WEEKS = 12                  # 마지막 12주 검증\n",
    "LEAKY = {\"actual_order_qty\", \"share_norm\", \"promo_flag\"}  # 누수 위험 피처는 제거하고 가져오기\n",
    "\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"Exists? TR:\", TR.exists(), \"TE:\", TE.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ce8d6-1f38-41ec-bb38-7c6bf3904931",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "LightGBM 모델에 넣을 최종 학습 데이터셋을 만드는 전처리 단계\n",
    "1. 누수 제거\n",
    "2. 이전 주 플래그 생성\n",
    "3. 수치형 피처만 자동 선택\n",
    "4. ID/타깃 제거\n",
    "5. 결측치 처리\n",
    "6. 상수/저분산 피처 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ce2f8-b23c-43af-a7ad-1739bc53ae63",
   "metadata": {},
   "source": [
    "## 모델 성능을 평가하기 위한 대칭 MAPE, 일반 MAPE 계산 함수\n",
    "- 수요 데이터는 0값이 많음 -> 분모가 0이 되는 문제를 처리해야함. denom == 0이면 1로 대체.\n",
    "- 예측 오차 비율을 직관적으로 보여줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591749c6-eea5-41af-9555-ee9aaa411f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff  = np.abs(y_true - y_pred)\n",
    "    denom = np.where(denom==0, 1.0, denom)\n",
    "    return float(np.mean(diff / denom) * 100.0)\n",
    "def mape(y_true, y_pred):\n",
    "    denom = np.where(y_true==0, 1.0, y_true)\n",
    "    return float((np.abs((y_true - y_pred) / denom)).mean() * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59c071-8247-407c-82a3-ad665e26042f",
   "metadata": {},
   "source": [
    "## 데이터 누수 방지\n",
    "- 사용 데이터의 프로모션 관련\n",
    "- INPUT : 지난주에 프로모션 했는가?\n",
    "- 이번 주 프로모션 여부인 promo_flag는 누수라 제거\n",
    "- SKU 단위로 시계열 정렬되어 있다고 가정\n",
    "- shift(1)로 한 주 미루기(데이터는 일주일 단위임) -> 지난주 값을 만듦\n",
    "- 첫번째 row는 NaN임 -> 0으로 채우기\n",
    "### 수요 예측 모델에서 현재 주의 실제 프로모션 여부를 피처로 넣으면 매래를 보는 누수 발생. -> 이전주만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed282976-5be5-4122-a27b-c991c04e0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"promo_flag\" in df.columns:\n",
    "        df[\"promo_flag_prev\"] = (\n",
    "            df.groupby([\"warehouse_id\",\"store_id\",\"sku_id\"])[\"promo_flag\"]\n",
    "              .shift(1).fillna(0)\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c72d5b-173f-4124-a783-a97943b329ca",
   "metadata": {},
   "source": [
    "## 모델 입력 피처 자동 생성\n",
    "- 훈련 데이터에서 모델 입력으로 사용할 컬럼만 자동 선택\n",
    "### 제외 컬럼\n",
    "1. ID_KEYS  \n",
    "   i. 그룹 구분용\n",
    "2. actual_order_qty  \n",
    "   i. y와 동일한 정보 -> 누수 및 과적합의 원인\n",
    "3. TARGET(\"y\")  \n",
    "    i. 모델 타깃\n",
    "4. 누수 위험 컬럼  \n",
    "   i. share_norm : 실제 판매량 기반 비율이니깐 누수 요소임\n",
    "   ii. promo_flag : 플래그\n",
    "\n",
    "### 포함 컬럼\n",
    "1. 수치형\n",
    "   i. number, bool\n",
    "2. 위의 제외 컬럼이 아닌것\n",
    "3. 이전 주 플래그\n",
    "   i. promo_flag_prev : promo_flag와 달리 이전 주 플래그\n",
    "\n",
    "### 전처리 파이프라인 + 피처 자동 관리\n",
    "### 새 컬럼이 생기거나 없어져도 자동으로 모델 적응"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc9e5a7-e7ae-4c2b-8866-2da2e45a3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df: pd.DataFrame) -> list:\n",
    "    ignore = set(ID_KEYS + [TARGET, \"actual_order_qty\"])\n",
    "    numeric_cols = set(df.select_dtypes(include=[\"number\",\"bool\"]).columns.tolist())\n",
    "    feats = [c for c in df.columns if c in numeric_cols and c not in ignore and c not in LEAKY]\n",
    "    if \"promo_flag_prev\" in df.columns and \"promo_flag_prev\" not in feats:\n",
    "        feats.append(\"promo_flag_prev\")\n",
    "    # 중복 제거\n",
    "    feats = list(dict.fromkeys(feats))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9624a-5ec0-4ab7-b683-6b4bbdb1ddf6",
   "metadata": {},
   "source": [
    "## 저분산/상수 피처 제거\n",
    "- 정보량이 없는 피처를 제거  \n",
    "  -> 과적합 방지  \n",
    "  -> 모델 효율 계선  \n",
    "  -> noisy 피처 감소  \n",
    "  \n",
    "- 제거 기준\n",
    "1. unique == 1\n",
    "2. 분산이 0에 가까움 -> 거의 변하지 않는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ccf389-e966-48c3-be3b-ae8020ab515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_variance(df_train: pd.DataFrame, cols, nunique_thresh=1, var_thresh=1e-8):\n",
    "    nun = df_train[cols].nunique()\n",
    "    low_nun = nun[nun <= nunique_thresh].index.tolist()\n",
    "    var = df_train[cols].var(numeric_only=True).fillna(0.0)\n",
    "    low_var = var[var <= var_thresh].index.tolist()\n",
    "    drop = sorted(set(low_nun + low_var))\n",
    "    keep = [c for c in cols if c not in drop]\n",
    "    return keep, drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533dad17-059b-4c87-90bd-711f475d75a5",
   "metadata": {},
   "source": [
    "## 실제 데이터 로딩/이전 주 플래그 생성/피처 자동 선택\n",
    "- train / test dataset을 날짜 파싱하여 로드\n",
    "- train과 test 모두 동일한 규칙으로 전처리\n",
    "- 훈련 데이터 기준으로 feature set 결정\n",
    "- test도 동일 feature를 사용하여 일관성 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d7d49b-8dac-4adc-9f69-da9e73c38d00",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\chosw\\\\ORDER101_114_AI\\\\be18-fin-SYNERGY-ORDER101\\\\python-server\\\\app\\\\data_pipeline\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tr = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m te = pd.read_csv(TE, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      4\u001b[39m tr = add_prev_flags(tr); te = add_prev_flags(te)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\chosw\\\\ORDER101_114_AI\\\\be18-fin-SYNERGY-ORDER101\\\\python-server\\\\app\\\\data_pipeline\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "tr = pd.read_csv(TR, parse_dates=[\"target_date\"])\n",
    "te = pd.read_csv(TE, parse_dates=[\"target_date\"])\n",
    "\n",
    "tr = add_prev_flags(tr); te = add_prev_flags(te)\n",
    "features = build_features(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e25745-65b8-48d0-98cc-6c817c883a71",
   "metadata": {},
   "source": [
    "## 결측치 처리\n",
    "- LightGBM의 안정적인 입력을 위한 결측치 처리.\n",
    "- 규칙\n",
    "float/int → 0.0  \n",
    "bool / int형 -> 0  \n",
    "train/test 모두 동일 규칙 적용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08f334-39e6-4dbb-94ea-a0c89f68942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in features:\n",
    "    if tr[c].dtype.kind in \"fi\":\n",
    "        tr[c] = tr[c].fillna(0.0); te[c] = te[c].fillna(0.0)\n",
    "    else:\n",
    "        tr[c] = tr[c].fillna(0);   te[c] = te[c].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71316667-4ed1-43ab-b8aa-5748b5b8e432",
   "metadata": {},
   "source": [
    "## 상수/저분산 피처 제거 후 최종 feature 리스트 확정\n",
    "- 훈련 기준으로 제거  \n",
    "- test에서도 동일한 feature set 사용해야함 -> feature만 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45e18b-b582-4275-a81a-c9518703b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, dropped = drop_low_variance(tr, features)\n",
    "print(\"사용 피처 수:\", len(features))\n",
    "print(features)\n",
    "if dropped:\n",
    "    print(\"제거된 상수/저분산 피처:\", dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fbb997-59f5-4c7b-b8ab-73f1c9f5f4e8",
   "metadata": {},
   "source": [
    "# 시계열 검증 분리\n",
    "- 마지막 12주를 검증 세트로\n",
    "- keys: 하나의 시계열을 정의하는 그룹 단위\n",
    "- sort_values(keys + [\"target_date\"])  \n",
    "각 시계열이 시간순으로 정렬되어 있어야 “마지막 12주”를 잘 뽑을 수 있음.\n",
    "- 정렬 기준:  \n",
    "warehouse_id  \n",
    "store_id  \n",
    "sku_id  \n",
    "target_date  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418d89d8-0a60-4193-accd-a68a2ec9a468",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m keys = [\u001b[33m\"\u001b[39m\u001b[33mwarehouse_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstore_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msku_id\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tr2 = \u001b[43mtr\u001b[49m.sort_values(keys + [\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m]).copy()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 각 그룹마다 시계열 인덱스 부여 (0, 1, 2...)\u001b[39;00m\n\u001b[32m      4\u001b[39m tr2[\u001b[33m\"\u001b[39m\u001b[33midx_in_grp\u001b[39m\u001b[33m\"\u001b[39m] = tr2.groupby(keys).cumcount()\n",
      "\u001b[31mNameError\u001b[39m: name 'tr' is not defined"
     ]
    }
   ],
   "source": [
    "keys = [\"warehouse_id\",\"store_id\",\"sku_id\"]\n",
    "tr2 = tr.sort_values(keys + [\"target_date\"]).copy()\n",
    "# 각 그룹마다 시계열 인덱스 부여 (0, 1, 2...)\n",
    "tr2[\"idx_in_grp\"] = tr2.groupby(keys).cumcount()\n",
    "# 각 그룹의 사이즈를 같은 그룹의 모든 row에 복사\n",
    "tr2[\"grp_size\"]   = tr2.groupby(keys)[TARGET].transform(\"size\")\n",
    "tr2[\"is_val\"]     = (tr2[\"grp_size\"] - tr2[\"idx_in_grp\"]) <= VAL_WEEKS\n",
    "tr_fit = tr2.loc[~tr2[\"is_val\"]].drop(columns=[\"idx_in_grp\",\"grp_size\",\"is_val\"])\n",
    "tr_val = tr2.loc[ tr2[\"is_val\"]].drop(columns=[\"idx_in_grp\",\"grp_size\",\"is_val\"])\n",
    "\n",
    "X_tr, y_tr = tr_fit[features], tr_fit[TARGET]\n",
    "X_va, y_va = tr_val[features], tr_val[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45190489-2e4f-4b01-b24e-55760bb62def",
   "metadata": {},
   "source": [
    "## LightGBM 모델 설정\n",
    "- objective=\"poisson\"  \n",
    "  수요가 카운트 데이터 성격일때 자주 사용함    \n",
    "  포아송 회귀는 비음수가 기본 가정.   \n",
    "  수요 같은 스케일에 맞는 경우가 많아서 자연스러움   \n",
    "- n_estimators=3000, learning_rate=0.02   \n",
    "    부스팅 트리 개수(n_estimators)  \n",
    "    한 번에 배우는 속도(learning_rate)  \n",
    "    learning_rate를 낮게(0.02) 두고, n_estimators를 크게(3000) 둔 뒤,  \n",
    "    early_stopping으로 적절한 트리 개수에서 stop 시켜서 안정적인 학습을 하고  \n",
    "    과적합을 방지함  \n",
    "- num_leaves=31, max_depth=8  \n",
    "    num_leaves: 한 트리에서 만들 수 있는 리프 노드 최대 개수  \n",
    "    클수록 복잡한 패턴을 잘 잡지만, 과적합  높음  \n",
    "    max_depth: 각 트리의 최대 깊이  \n",
    "    8이면 그리 깊지는 않지만, 그래도 어느 정도 비선형성은 표현 가능  \n",
    "- min_child_samples=128  \n",
    "    리프 노드 하나가 최소 몇개의 데이터를 가져야 하는지  \n",
    "    값이 너무 크면 너무 소수인 데이터에 특화된 리프는 과적합을 가져와서 방지함  \n",
    "    대신 세밀한 패턴을 덜 캐치함  \n",
    "    -> 수요 예측은 데이터가 많고 노이즈도 많음  \n",
    "- min_gain_to_split=0.02  \n",
    "    트리를 분할할 때 얻는 gain이   \n",
    "    0.02보다 작으면 더 이상 쪼개지 않도록 제한    \n",
    "    너무 쓸데없는 분할(미미한 개선) 방지함 -> 일반화 성능에 도움.  \n",
    "### 샘플링  \n",
    "행 + 열을 둘다 랜덤 샘플링  \n",
    "조금씩 다른 관점을 가진 트릴를 여러개 만들도록 한 구조임  \n",
    "- subsample=0.65   \n",
    "    각 트리를 만들 때 전체 데이터 중 65%만 랜덤 샘플링해서 사용.   \n",
    "    과적합 방지용 (랜덤성 부여)  \n",
    "- subsample_freq=1  \n",
    "    매 트리마다 저 샘플링을 적용  \n",
    "- colsample_bytree=0.65  \n",
    "    각 트리를 만들 때 피처도 65%만 랜덤 선택해서 사용.  \n",
    "### 규제  \n",
    "- reg_alpha=2.0, reg_lambda=2.0  \n",
    "    reg_alpha: L1 규제 (가중치에 절댓값 페널티)    \n",
    "    reg_lambda: L2 규제 (L2, Ridge 스타일)    \n",
    "    둘 다 2.0으로 준 건 꽤 강한 편의 정규화라서 과적합 감소  \n",
    "    가중치가 너무 극단적으로 치우치는 것 방지  \n",
    "### 재현성 & 로그  \n",
    "- random_state=42, verbosity=-1    \n",
    "    random_state=42: 결과 재현성을 위해 랜덤 시드 고정.  \n",
    "    verbosity=-1: 학습 과정에서 불필요한 LightGBM 로그 숨김.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac761bda-9c14-497f-b0ce-6887227bdf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(\n",
    "    objective=\"poisson\",\n",
    "    n_estimators=3000, learning_rate=0.02,\n",
    "    num_leaves=31, max_depth=8,\n",
    "    min_child_samples=128,\n",
    "    min_gain_to_split=0.02,\n",
    "    subsample=0.65, subsample_freq=1,\n",
    "    colsample_bytree=0.65,\n",
    "    reg_alpha=2.0, reg_lambda=2.0,\n",
    "    random_state=42, verbosity=-1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633fbd0-6d35-4d66-9a12-69b9c5f87727",
   "metadata": {},
   "source": [
    "## 학습 + 얼리스타핑 설정\n",
    "평가 지표로는 MAE(평균 절대 오차)를 사용\n",
    "얼리스타핑으로 적절한 트리 개수를 자동으로 찾는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66b5029-005a-4118-94ed-1bd6ceab35a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model.fit(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mX_tr\u001b[49m, y_tr,\n\u001b[32m      3\u001b[39m     eval_set=[(X_va, y_va)],\n\u001b[32m      4\u001b[39m     eval_metric=\u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     callbacks=[early_stopping(\u001b[32m15\u001b[39m, first_metric_only=\u001b[38;5;28;01mTrue\u001b[39;00m), log_evaluation(\u001b[32m200\u001b[39m)]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m best_iter = \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mbest_iteration_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbest_iteration:\u001b[39m\u001b[33m\"\u001b[39m, best_iter)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_va, y_va)],\n",
    "    eval_metric=\"mae\",\n",
    "    callbacks=[early_stopping(15, first_metric_only=True), log_evaluation(200)]\n",
    ")\n",
    "\n",
    "best_iter = getattr(model, \"best_iteration_\", None)\n",
    "print(\"best_iteration:\", best_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e2e41-cf2e-4567-8b8a-cb59a9b6a394",
   "metadata": {},
   "source": [
    "# 학습된 모델로 test셋 전체 평가 및 SKU별로 성능 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887878b-7c40-4962-9115-5d3b84fe81da",
   "metadata": {},
   "source": [
    "- 테스트셋 예측/클리핑\n",
    "- 테스트셋 전체 성능 평가\n",
    "- SKU별 집계\n",
    "\n",
    "1. 테스트셋 전체에 대해 예측 수행\n",
    "2. 예측 결과를 0 이상으로 클리핑해서 현실적인 값으로 맞춘다\n",
    "3. 테스트셋 전체 기준으로 MAE/MAPE/SMAPE 총괄 성능 계산\n",
    "4. sku_is 단위로 다시 그룹핑해서 SKU별 데이터 개수와 SKU별 MAE를 구해서 상위 SKU들 기준으로 모델이 괜찮게 작동하는지 확인\n",
    "\n",
    "\n",
    "클리핑 : 예측값이 음수가 나오면 강제로 0으로\n",
    "- MAE : 평균 절대 오차   \n",
    "MAE = 평균(|실제값 - 예측값|)\n",
    "- MAPE : 평균 절대 백분율 오차  \n",
    "MAPE = 평균( |실제 - 예측| / 실제 ) × 100%  \n",
    "- SMAPE : MAPE의 문제(분모를 실제값만 사용하는 왜곡)을 해결한 버전   \n",
    "SMAPE = 평균( |y_pred - y_true| / ((|y_pred| + |y_true|)/2) ) × 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24774fa-bd44-4285-9b1e-645b74e83389",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m te_pred = np.clip(model.predict(\u001b[43mte\u001b[49m[features], num_iteration=best_iter), \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      2\u001b[39m te = te.copy()\n\u001b[32m      3\u001b[39m te[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m] = te_pred\n",
      "\u001b[31mNameError\u001b[39m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "te_pred = np.clip(model.predict(te[features], num_iteration=best_iter), 0, None)\n",
    "te = te.copy()\n",
    "te[\"y_pred\"] = te_pred\n",
    "\n",
    "mae   = mean_absolute_error(te[TARGET], te_pred)\n",
    "_mape = mape(te[TARGET].values, te_pred)\n",
    "_smape= smape(te[TARGET].values, te_pred)\n",
    "print(f\"MAE={mae:.4f} | MAPE={_mape:.2f}% | SMAPE={_smape:.2f}% (n={len(te)})\")\n",
    "\n",
    "sku_eval = (\n",
    "    te.groupby(\"sku_id\")\n",
    "      .agg(n=(\"y\",\"size\"), mae=(\"y\", lambda s: float(np.mean(np.abs(s - te.loc[s.index, \"y_pred\"])))))\n",
    "      .sort_values(\"n\", ascending=False)\n",
    ")\n",
    "sku_eval.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc5e08-7175-48f4-8331-5b59217251a0",
   "metadata": {},
   "source": [
    "# actual vs predict\n",
    "SKU를 임의로 골라서 날짜별 실제 데이터와 예측 시계열을 확인 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5719d8-350a-4089-84ac-162b8b66d943",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\chosw\\\\ORDER101_114_AI\\\\be18-fin-SYNERGY-ORDER101\\\\python-server\\\\app\\\\data_pipeline\\\\features_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     54\u001b[39m     plt.tight_layout()\n\u001b[32m     55\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVAC-XIA-ROBO-2024\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m test(\u001b[33m\"\u001b[39m\u001b[33mAC-CAR-16P-2024\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m test(\u001b[33m\"\u001b[39m\u001b[33mAC-LG-STAND-18P-2024\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(SKU)\u001b[39m\n\u001b[32m      6\u001b[39m PRED_PATH     = BASE / \u001b[33m\"\u001b[39m\u001b[33mpredictions.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#SKU           = \"VAC-XIA-ROBO-2024\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- load ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m feat = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFEATURES_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m pred = pd.read_csv(PRED_PATH,     parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# --- filter SKU ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\chosw\\\\ORDER101_114_AI\\\\be18-fin-SYNERGY-ORDER101\\\\python-server\\\\app\\\\data_pipeline\\\\features_test.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def test(SKU):\n",
    "    # --- file paths (edit if needed) ---\n",
    "    FEATURES_PATH = BASE / \"features_test.csv\"\n",
    "    PRED_PATH     = BASE / \"predictions.csv\"\n",
    "    #SKU           = \"VAC-XIA-ROBO-2024\"\n",
    "    \n",
    "    # --- load ---\n",
    "    feat = pd.read_csv(FEATURES_PATH, parse_dates=[\"target_date\"])\n",
    "    pred = pd.read_csv(PRED_PATH,     parse_dates=[\"target_date\"])\n",
    "    \n",
    "    # --- filter SKU ---\n",
    "    feat_sku = feat[feat[\"sku_id\"] == SKU].copy()\n",
    "    pred_sku = pred[pred[\"sku_id\"] == SKU].copy()\n",
    "    \n",
    "    # --- aggregate by date in case there are multiple rows per target_date (e.g., by store/region) ---\n",
    "    # features_test.csv: we expect \"actual_order_qty\"\n",
    "    actual_by_day = (\n",
    "        feat_sku.groupby(\"target_date\", as_index=False)[\"actual_order_qty\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"actual_order_qty\": \"y_actual\"})\n",
    "    )\n",
    "    \n",
    "    # predictions.csv: we expect \"y_pred\"\n",
    "    pred_by_day = (\n",
    "        pred_sku.groupby(\"target_date\", as_index=False)[\"y_pred\"]\n",
    "        .sum()\n",
    "    )\n",
    "    \n",
    "    # --- align by common dates (inner join) ---\n",
    "    df = pd.merge(actual_by_day, pred_by_day, on=\"target_date\", how=\"inner\").sort_values(\"target_date\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No overlapping dates found for SKU={SKU}. \"\n",
    "                         \"Check that both files contain this SKU and matching target_date values.\")\n",
    "    \n",
    "    # --- (optional) quick metrics ---\n",
    "    try:\n",
    "        mae = (df[\"y_actual\"] - df[\"y_pred\"]).abs().mean()\n",
    "        print(f\"MAE for {SKU}: {mae:.3f} over {len(df)} days\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute MAE:\", e)\n",
    "    \n",
    "    # --- plot ---\n",
    "    plt.figure(figsize=(11, 4))\n",
    "    plt.plot(df[\"target_date\"], df[\"y_actual\"], label=\"Actual (actual_order_qty)\")\n",
    "    plt.plot(df[\"target_date\"], df[\"y_pred\"],   label=\"Predicted (y_pred)\")\n",
    "    plt.title(f\"{SKU} — Actual vs Predicted by Date\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Quantity\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test(\"VAC-XIA-ROBO-2024\")\n",
    "test(\"AC-CAR-16P-2024\")\n",
    "test(\"AC-LG-STAND-18P-2024\")\n",
    "test(\"MON-DEL-24FHD-2023\")\n",
    "test(\"WM-MIE-DRUM-9K-2023\")\n",
    "test(\"MIX-CUI-HAND-2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc439c94-80b5-48dc-95af-c624589b6a54",
   "metadata": {},
   "source": [
    "# 결과 저장\n",
    "최종적으로는 4개의 파일이 여기서 생성됨\n",
    "| 파일                         | 내용                              | 용도                         |\n",
    "| -------------------------- | ------------------------------- | -------------------------- |\n",
    "| predictions.csv        | 실제값 + 예측값                       | 성능 분석, 그래프, 비교 분석          |\n",
    "| lightgbm_model.pkl     | 학습된 모델 자체                       | FastAPI 추론 API, 재학습 없이 재사용 |\n",
    "| lightgbm_features.json | 모델 입력에 사용된 컬럼 목록                | 재현성 확보, 추론 시 동일 피처 사용      |\n",
    "| metrics_eval.csv       | MAE/MAPE/SMAPE + best_iteration | 모델 버전 관리, 성능 비교            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d3b03dd-4ee2-47a9-9957-802a6627b33b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 예측 저장\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m keep_ids = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mwarehouse_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstore_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msku_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mregion\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mte\u001b[49m.columns]\n\u001b[32m      3\u001b[39m out = te[keep_ids + [TARGET, \u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m]].copy()\n\u001b[32m      4\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m] = out[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m].round(\u001b[32m0\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "# 예측 저장\n",
    "keep_ids = [c for c in [\"warehouse_id\",\"store_id\",\"sku_id\",\"region\",\"target_date\"] if c in te.columns]\n",
    "out = te[keep_ids + [TARGET, \"y_pred\"]].copy()\n",
    "out[\"y_pred\"] = out[\"y_pred\"].round(0).astype(int)\n",
    "out.to_csv(OUT_PRED, index=False)\n",
    "\n",
    "# 모델/피처/메트릭 저장\n",
    "import joblib\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "with open(FEATS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(features, f, ensure_ascii=False, indent=2)\n",
    "pd.DataFrame({\n",
    "    \"metric\":[\"mae\",\"mape\",\"smape\"],\n",
    "    \"value\":[mae, _mape, _smape],\n",
    "    \"best_iteration\":[best_iter]*3\n",
    "}).to_csv(METRICS_CSV, index=False)\n",
    "\n",
    "print(\"saved:\", OUT_PRED.name, \"| model:\", MODEL_PATH.name, \"| feats:\", FEATS_JSON.name, \"| metrics:\", METRICS_CSV.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6814057-fc1f-4f8c-bd32-215859423275",
   "metadata": {},
   "source": [
    "# 과적합 원인 찾기\n",
    "overlap rows가 0이여야 과적합이 아니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffdd8664-7b75-4220-9eff-363b677b755c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 과적합 원인 찾기\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain date range:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtr\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m].min(), \u001b[33m\"\u001b[39m\u001b[33m→\u001b[39m\u001b[33m\"\u001b[39m, tr[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m].max())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest  date range:\u001b[39m\u001b[33m\"\u001b[39m, te[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m].min(), \u001b[33m\"\u001b[39m\u001b[33m→\u001b[39m\u001b[33m\"\u001b[39m, te[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m].max())\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 날짜 겹치는지\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tr' is not defined"
     ]
    }
   ],
   "source": [
    "# 과적합 원인 찾기\n",
    "print(\"Train date range:\", tr[\"target_date\"].min(), \"→\", tr[\"target_date\"].max())\n",
    "print(\"Test  date range:\", te[\"target_date\"].min(), \"→\", te[\"target_date\"].max())\n",
    "\n",
    "# 날짜 겹치는지\n",
    "overlap = pd.merge(\n",
    "    tr[[\"warehouse_id\",\"store_id\",\"sku_id\",\"target_date\"]],\n",
    "    te[[\"warehouse_id\",\"store_id\",\"sku_id\",\"target_date\"]],\n",
    "    on=[\"warehouse_id\",\"store_id\",\"sku_id\",\"target_date\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "print(\"overlap rows:\", len(overlap))\n",
    "\n",
    "# 피처에 누수 의심 변수 포함되어 있는지\n",
    "suspect_cols = [c for c in features if any(k in c.lower() for k in [\"order\", \"qty\", \"actual\", \"target\"])]\n",
    "print(\"누수 의심 피처:\", suspect_cols)\n",
    "\n",
    "# 이동평균 피처 확인\n",
    "ma_cols = [c for c in features if \"ma\" in c.lower()]\n",
    "print(\"이동평균 피처:\", ma_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad1aa2-7c38-4134-afac-7202a7c7de5a",
   "metadata": {},
   "source": [
    "# 예측 생성/ 테스트셋 정렬\n",
    "y_pred가 없으면 예측 생성  \n",
    "테스트셋 기본 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db54363-c2d3-4cd1-af0d-f22a1763a280",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m leaky = {\u001b[33m\"\u001b[39m\u001b[33mactual_order_qty\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mshare_norm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpromo_flag\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# 누출 방지\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- (1) promo_flag_prev 안전 생성: 없을 때만 생성 ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df_ \u001b[38;5;129;01min\u001b[39;00m (\u001b[43mtr\u001b[49m, te):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpromo_flag\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpromo_flag_prev\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df_.columns:\n\u001b[32m     12\u001b[39m         df_[\u001b[33m\"\u001b[39m\u001b[33mpromo_flag_prev\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m     13\u001b[39m             df_.groupby([\u001b[33m\"\u001b[39m\u001b[33mwarehouse_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstore_id\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msku_id\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[33m\"\u001b[39m\u001b[33mpromo_flag\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     14\u001b[39m               .shift(\u001b[32m1\u001b[39m).fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     15\u001b[39m         )\n",
      "\u001b[31mNameError\u001b[39m: name 'tr' is not defined"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "target = \"y\"\n",
    "ignore = [\"warehouse_id\",\"store_id\",\"sku_id\",\"region\",\"target_date\",\"split\",target]\n",
    "leaky = {\"actual_order_qty\", \"share_norm\", \"promo_flag\"}  # 누출 방지\n",
    "\n",
    "# --- (1) promo_flag_prev 안전 생성: 없을 때만 생성 ---\n",
    "for df_ in (tr, te):\n",
    "    if \"promo_flag\" in df_.columns and \"promo_flag_prev\" not in df_.columns:\n",
    "        df_[\"promo_flag_prev\"] = (\n",
    "            df_.groupby([\"warehouse_id\",\"store_id\",\"sku_id\"])[\"promo_flag\"]\n",
    "              .shift(1).fillna(0)\n",
    "        )\n",
    "\n",
    "# --- (2) 피처 선택 + 중복 제거 ---\n",
    "numeric_cols = tr.select_dtypes(include=[\"number\",\"bool\"]).columns.tolist()\n",
    "features = [c for c in numeric_cols if c not in ignore and c not in leaky]\n",
    "if \"promo_flag_prev\" in tr.columns:  # lag된 프로모만 사용\n",
    "    features.append(\"promo_flag_prev\")\n",
    "# 중복 제거\n",
    "features = list(dict.fromkeys(features))\n",
    "\n",
    "print(f\"사용 피처 수: {len(features)}개\")\n",
    "print(features)\n",
    "\n",
    "# NaN 처리\n",
    "for c in features:\n",
    "    if tr[c].dtype.kind in \"fi\":\n",
    "        tr[c] = tr[c].fillna(0.0); te[c] = te[c].fillna(0.0)\n",
    "    else:\n",
    "        tr[c] = tr[c].fillna(0);   te[c] = te[c].fillna(0)\n",
    "\n",
    "# 학습\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1200, learning_rate=0.01,\n",
    "    subsample=0.9, colsample_bytree=0.8,\n",
    "    num_leaves=31, random_state=42\n",
    ")\n",
    "model.fit(tr[features], tr[target])\n",
    "\n",
    "# 예측/평가\n",
    "te_pred = np.clip(model.predict(te[features]), 0, None)\n",
    "mae  = mean_absolute_error(te[target], te_pred)\n",
    "mape = (np.abs(te[target] - te_pred) / np.maximum(te[target], 1)).mean() * 100\n",
    "print(f\"MAE={mae:.4f}, MAPE={mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bd088-df4c-424b-b03a-0e2b497a19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target = \"y\"\n",
    "\n",
    "# 아직 예측 안했다면 생성\n",
    "if \"y_pred\" not in te.columns:\n",
    "    te[\"y_pred\"] = np.clip(model.predict(te[features]), 0, None)\n",
    "\n",
    "# 기본 정렬\n",
    "te = te.sort_values([\"warehouse_id\",\"store_id\",\"sku_id\",\"target_date\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9706d1-b73e-455e-a2f0-18a646c82b70",
   "metadata": {},
   "source": [
    "# 모델 성능 파악  \n",
    "SKU 단위로 모델 성능을 보는 단계    \n",
    "커스텀 mape 사용함  -> 실제값이 0일때 MAPE는 무한대가 나와서  \n",
    "SKU 단위로 모아서 지표 계산    \n",
    "출력은 테스트 기간동안 판매량이 제일 많고 정확도가 상대적으로 높은 상위 10개 SKU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178c8915-259c-4027-8037-1ca413ee00e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     denom = np.maximum(a, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# 0-division 방지\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (np.abs(a - p) / denom).mean() * \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sku_eval = (\u001b[43mte\u001b[49m\n\u001b[32m      7\u001b[39m     .groupby(\u001b[33m\"\u001b[39m\u001b[33msku_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     .apply(\u001b[38;5;28;01mlambda\u001b[39;00m g: pd.Series({\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(g),\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mqty_sum\u001b[39m\u001b[33m\"\u001b[39m: g[target].sum(),\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m: np.abs(g[target]-g[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m]).mean(),\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmape\u001b[39m\u001b[33m\"\u001b[39m: mape(g[target].values, g[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m].values)\n\u001b[32m     13\u001b[39m     }))\n\u001b[32m     14\u001b[39m     .sort_values([\u001b[33m\"\u001b[39m\u001b[33mqty_sum\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmape\u001b[39m\u001b[33m\"\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m])\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m sku_eval.head(\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "# SKU 단위 MAE / MAPE 요약\n",
    "def mape(a, p):\n",
    "    denom = np.maximum(a, 1)  # 0-division 방지\n",
    "    return (np.abs(a - p) / denom).mean() * 100\n",
    "\n",
    "sku_eval = (te\n",
    "    .groupby(\"sku_id\")\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"n\": len(g),\n",
    "        \"qty_sum\": g[target].sum(),\n",
    "        \"mae\": np.abs(g[target]-g[\"y_pred\"]).mean(),\n",
    "        \"mape\": mape(g[target].values, g[\"y_pred\"].values)\n",
    "    }))\n",
    "    .sort_values([\"qty_sum\",\"mape\"], ascending=[False, True])\n",
    ")\n",
    "sku_eval.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920743d-5ebd-4efd-b38d-8be8ced008d8",
   "metadata": {},
   "source": [
    "# 잔차 히스토그램/ Scatter Plot\n",
    "### 잔차 히스토그램\n",
    "각 테스트 데이터에 대해 실제값 - 예측값인 잔차값을 히스토그램으로 그림  \n",
    "오차 분포 시각화\n",
    "### Actual vs Predicted Scatter Plot\n",
    "x축 = 실제값  \n",
    "y축 = 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74419427-1988-4f99-8455-c68dacd3cfc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mte\u001b[49m[target] - te[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m6\u001b[39m,\u001b[32m4\u001b[39m))\n\u001b[32m      4\u001b[39m plt.hist(res, bins=\u001b[32m40\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "res = te[target] - te[\"y_pred\"]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(res, bins=40)\n",
    "plt.title(\"Residuals Distribution (y - y_pred)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(te[target], te[\"y_pred\"], s=8, alpha=0.4)\n",
    "plt.plot([0, te[[target,'y_pred']].max().max()],\n",
    "         [0, te[[target,'y_pred']].max().max()],\n",
    "         linestyle=\"--\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec181d6-dede-402d-8fd5-84e14f4262cb",
   "metadata": {},
   "source": [
    "# 월별 모델 확인\n",
    "월별로 모델이 잘 맞는지 or 특정 달에만 폭망하는지 보는것    \n",
    "1. 과적합/언더핏을 “시간 축”에서 보는 진단 도구  \n",
    "단순 전체 MAE/SMAPE만 보면 놓치는 패턴을 캐치 가능    \n",
    "2. 계절성(seasonality) 문제점 파악 가능  \n",
    "여름/겨울/명절 달에서만 터지는 오차를 체크     \n",
    "3. 외부요인(feature 추가)의 효과를 검증하기 딱 좋은 구조    \n",
    "before/after 두 버전 모델로 똑같은 그래프 그려서 비교하면 됨 \n",
    "여름/겨울/명절 달에서만 터지는 오차를 체크   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09eacd96-afac-460c-951f-d14e4da3196f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tmp = \u001b[43mte\u001b[49m.copy()\n\u001b[32m      2\u001b[39m tmp[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(tmp[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m]).dt.year\n\u001b[32m      3\u001b[39m tmp[\u001b[33m\"\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(tmp[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m]).dt.month\n",
      "\u001b[31mNameError\u001b[39m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = te.copy()\n",
    "tmp[\"year\"] = pd.to_datetime(tmp[\"target_date\"]).dt.year\n",
    "tmp[\"month\"] = pd.to_datetime(tmp[\"target_date\"]).dt.month\n",
    "by_month = (tmp.groupby([\"year\",\"month\"])\n",
    "              .apply(lambda g: pd.Series({\n",
    "                  \"n\": len(g),\n",
    "                  \"mae\": np.abs(g[target]-g[\"y_pred\"]).mean(),\n",
    "                  \"mape\": mape(g[target].values, g[\"y_pred\"].values)\n",
    "              }))\n",
    "              .reset_index())\n",
    "display(by_month.sort_values([\"year\",\"month\"]).head(24))\n",
    "\n",
    "# 월별 MAPE 라인\n",
    "pivot = by_month.pivot(index=\"month\", columns=\"year\", values=\"mape\")\n",
    "pivot.plot(figsize=(9,4), marker=\"o\")\n",
    "plt.title(\"MAPE by Month\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e4121-51a6-4798-a38f-08eb40980a64",
   "metadata": {},
   "source": [
    "# LightGBM feature importance 계산\n",
    "모델이 어떤 피처를 얼마나 중요하게 사용했는지를 시각화\n",
    "TOP20 중요 피처만 표로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c21547b-9d4c-43ec-82ed-2082061eedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 feature_importances_를 제공하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    fi = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8, max(4, 0.25*len(fi))))\n",
    "    plt.barh(fi[\"feature\"][::-1], fi[\"importance\"][::-1])\n",
    "    plt.title(\"LightGBM Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fi.head(20)\n",
    "else:\n",
    "    print(\"모델이 feature_importances_를 제공하지 않습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c15646-3e69-434e-a86b-1df8661fd640",
   "metadata": {},
   "source": [
    "# 상관 관계 체크\n",
    "원시 도메인 판매 데이터에서 SKU들끼리 서로 얼마나 같이 움직이는지 체크  \n",
    "1. domain_sales_sku.csv에서 SKU/날짜별 판매량 데이터를 불러옴\n",
    "2. 그걸 날짜 × SKU 매트릭스로 바꿈\n",
    "3. SKU들끼리 같은 날짜에서 판매량이 얼마나 같이 움직이는지 상관계수 구함\n",
    "4. 그 전체 상관값의 중앙값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80139031-34b3-4f01-aa8e-65682442d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU groups: 5400\n",
      "Pairwise corr median: 0.040372267439984545\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, pathlib as p\n",
    "candidates = [\n",
    "    Path.cwd() / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[1] / \"app\" / \"data_pipeline\",\n",
    "    Path(\"python-server/app/data_pipeline\").resolve(),\n",
    "]\n",
    "BASE = next((p for p in candidates if p.exists()), None)\n",
    "df = pd.read_csv(BASE/\"domain_sales_sku.csv\", parse_dates=[\"target_date\"])\n",
    "print(\"SKU groups:\", df[\"sku_id\"].nunique())\n",
    "\n",
    "# 같은 주차에서 SKU 간 상관이 너무 높지 않은지 대략 체크\n",
    "g = (df.groupby([\"sku_id\",\"target_date\"])[\"sku_qty\"].sum().unstack(0).fillna(0))\n",
    "print(\"Pairwise corr median:\", g.corr().stack().median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ccf7e72-2b09-4d34-9d75-53e9deb62ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sku_id       1  2\n",
      "target_date      \n",
      "2021-01-04   1  1\n",
      "2021-01-11   1  1\n",
      "2021-01-18   1  1\n",
      "2021-01-25   2  1\n",
      "2021-02-01   2  1\n"
     ]
    }
   ],
   "source": [
    "sids = df[\"sku_id\"].unique()[:2]\n",
    "print(df[df[\"sku_id\"].isin(sids)].pivot_table(\n",
    "    index=\"target_date\", columns=\"sku_id\", values=\"sku_qty\", aggfunc=\"sum\").dropna().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ccff6f-3774-442f-9769-624fe5caa71e",
   "metadata": {},
   "source": [
    "# 시계열 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a9a1f6-a098-4114-add7-1dc271e353f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'region'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m fa = pd.read_csv(BASE/\u001b[33m\"\u001b[39m\u001b[33mfeatures_all.csv\u001b[39m\u001b[33m\"\u001b[39m, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mtarget_date\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgroup ngroups:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mfa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwarehouse_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msku_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.ngroups)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:9190\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9196\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1330\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1341\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'region'"
     ]
    }
   ],
   "source": [
    "fa = pd.read_csv(BASE/\"features_all.csv\", parse_dates=[\"target_date\"])\n",
    "print(\"group ngroups:\", fa.groupby([\"warehouse_id\",\"store_id\",\"sku_id\",\"region\"]).ngroups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845159e1-2a03-4133-b901-cb6b43f36c79",
   "metadata": {},
   "source": [
    "# SKU 간에 상관 관계 파악\n",
    "데이터별로 추이가 같았기에 이를 수정 후 시각화해서 확인해보려고 작성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65628107-54cf-4392-bce4-a0b4e9adaf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "candidates = [\n",
    "    Path.cwd() / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[1] / \"app\" / \"data_pipeline\",\n",
    "    Path(\"python-server/app/data_pipeline\").resolve(),\n",
    "]\n",
    "BASE = next((p for p in candidates if p.exists()), None)\n",
    "\n",
    "TR = BASE / \"features_train.csv\"\n",
    "TE = BASE / \"features_test.csv\"\n",
    "OUT_PRED    = BASE / \"predictions.csv\"\n",
    "MODEL_PATH  = BASE / \"lightgbm_model.pkl\"\n",
    "FEATS_JSON  = BASE / \"lightgbm_features.json\"\n",
    "METRICS_CSV = BASE / \"metrics_eval.csv\"\n",
    "DOMAIN_SALE = BASE / \"domain_sales_sku.csv\"\n",
    "FEAT_ALL = BASE / \"features_all.csv\"\n",
    "\n",
    "\n",
    "\n",
    "ds = pd.read_csv(DOMAIN_SALE, parse_dates=[\"target_date\"])\n",
    "\n",
    "pick = ds[\"sku_id\"].drop_duplicates().sample(2, random_state=0).tolist()\n",
    "fig, axes = plt.subplots(2,1, figsize=(10,6), sharex=True)\n",
    "for ax, sid in zip(axes, pick):\n",
    "    s = (ds[ds.sku_id==sid]\n",
    "         .sort_values(\"target_date\")\n",
    "         .groupby(\"target_date\")[\"sku_qty\"].sum())\n",
    "    ax.plot(s.index, s.values); ax.set_title(sid)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 상관도(두 SKU가 너무 비슷하면 0.95↑)\n",
    "s1 = (ds[ds.sku_id==pick[0]].sort_values(\"target_date\")\n",
    "        .groupby(\"target_date\")[\"sku_qty\"].sum())\n",
    "s2 = (ds[ds.sku_id==pick[1]].sort_values(\"target_date\")\n",
    "        .groupby(\"target_date\")[\"sku_qty\"].sum())\n",
    "joined = pd.concat([s1, s2], axis=1).dropna()\n",
    "print(\"corr:\", joined.corr().iloc[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acac796-2ddd-4312-a58a-8f5c54499035",
   "metadata": {},
   "source": [
    "# 점검용\n",
    "오차 평균 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2b4cd-d93d-4f42-8afa-7b693c4a62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling_backtest.py (요약)\n",
    "import pandas as pd, numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import json, time, numpy as np, pandas as pd\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "candidates = [\n",
    "    Path.cwd() / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"data_pipeline\",\n",
    "    Path.cwd().parents[0] / \"app\" / \"data_pipeline\",\n",
    "    Path.cwd().parents[1] / \"app\" / \"data_pipeline\",\n",
    "    Path(\"python-server/app/data_pipeline\").resolve(),\n",
    "]\n",
    "BASE = next((p for p in candidates if p.exists()), None)\n",
    "\n",
    "TR = BASE / \"features_train.csv\"\n",
    "TE = BASE / \"features_test.csv\"\n",
    "OUT_PRED    = BASE / \"predictions.csv\"\n",
    "MODEL_PATH  = BASE / \"lightgbm_model.pkl\"\n",
    "FEATS_JSON  = BASE / \"lightgbm_features.json\"\n",
    "METRICS_CSV = BASE / \"metrics_eval.csv\"\n",
    "DOMAIN_SALE = BASE / \"domain_sales_sku.csv\"\n",
    "FEAT_ALL = BASE / \"features_all.csv\"\n",
    "LIGHTBGM_FEAT = BASE /\"lightgbm_features.json\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- params: 빠른 디버그용 ----------------\n",
    "H               = 12      # 검증 창 (주)\n",
    "MIN_TRAIN_WEEKS = 40\n",
    "STEP_WEEKS      = 8       # 오리진 간격\n",
    "MAX_SERIES      = 50      # 최대 시계열 개수\n",
    "MAX_SPLITS      = 5       # 시계열당 최대 스플릿 수 (뒤에서부터)\n",
    "MAX_SECONDS     = 120     # 전체 타임아웃(초)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "keys = [\"warehouse_id\",\"store_id\",\"sku_id\"]\n",
    "\n",
    "tr = pd.read_csv(TR, parse_dates=[\"target_date\"])\n",
    "te = pd.read_csv(TE, parse_dates=[\"target_date\"])\n",
    "df = pd.concat([tr, te], ignore_index=True).sort_values(keys + [\"target_date\"])\n",
    "\n",
    "# --- 학습 때와 동일한 파생 ---\n",
    "if \"promo_flag\" not in df.columns:\n",
    "    df[\"promo_flag\"] = (df.get(\"share_norm\", 0) > 0.25).astype(int)\n",
    "if \"promo_flag_prev\" not in df.columns:\n",
    "    df[\"promo_flag_prev\"] = (\n",
    "        df.groupby(keys)[\"promo_flag\"].shift(1).fillna(0).astype(int)\n",
    "    )\n",
    "\n",
    "# --- 피처 목록 로드 & 교집합 ---\n",
    "with open(FEATS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    features = [c for c in json.load(f) if c in df.columns]\n",
    "\n",
    "for c in features:\n",
    "    df[c] = df[c].fillna(0.0) if df[c].dtype.kind in \"fi\" else df[c].fillna(0)\n",
    "\n",
    "# --- 평가 대상 시계열 (상위 판매량 기준 일부만 샘플링) ---\n",
    "vol = (df.groupby(keys)[\"y\"].sum().sort_values(ascending=False))\n",
    "series_list = [tuple(ix) for ix in vol.head(MAX_SERIES).index]\n",
    "\n",
    "start = time.time()\n",
    "maes, n_cases = [], 0\n",
    "\n",
    "def iter_splits(g):\n",
    "    dmin, dmax = g[\"target_date\"].min(), g[\"target_date\"].max()\n",
    "    # 가능한 오리진들\n",
    "    origins = pd.date_range(\n",
    "        dmin + pd.Timedelta(weeks=MIN_TRAIN_WEEKS),\n",
    "        dmax - pd.Timedelta(weeks=H),\n",
    "        freq=f\"{STEP_WEEKS}W-MON\"\n",
    "    )\n",
    "    # 뒤에서부터 MAX_SPLITS개만\n",
    "    return list(origins)[-MAX_SPLITS:]\n",
    "\n",
    "for ser in series_list:\n",
    "    g = df[(df[\"warehouse_id\"]==ser[0]) & (df[\"store_id\"]==ser[1]) & (df[\"sku_id\"]==ser[2])]\n",
    "    if len(g) < (MIN_TRAIN_WEEKS + H + 1): \n",
    "        continue\n",
    "\n",
    "    for split in iter_splits(g):\n",
    "        if time.time() - start > MAX_SECONDS:\n",
    "            print(f\"[STOP] time budget {MAX_SECONDS}s reached.\")\n",
    "            break\n",
    "\n",
    "        train = g[g[\"target_date\"] < split]\n",
    "        val   = g[(g[\"target_date\"] >= split) &\n",
    "                  (g[\"target_date\"] <  split + pd.Timedelta(weeks=H))]\n",
    "        if len(train) < MIN_TRAIN_WEEKS or len(val) < H:\n",
    "            continue\n",
    "\n",
    "        m = LGBMRegressor(\n",
    "            n_estimators=1500,        \n",
    "            learning_rate=0.03,\n",
    "            num_leaves=31, max_depth=8,\n",
    "            subsample=0.8, colsample_bytree=0.7,\n",
    "            min_child_samples=40,    \n",
    "            reg_alpha=0.8, reg_lambda=1.0,\n",
    "            random_state=42, verbose=-1\n",
    "        )\n",
    "        m.fit(\n",
    "            train[features], train[\"y\"],\n",
    "            eval_set=[(val[features], val[\"y\"])],\n",
    "            eval_metric=\"mae\",\n",
    "            callbacks=[early_stopping(100, first_metric_only=True)]  # ES 단축\n",
    "        )\n",
    "        best_iter = getattr(m, \"best_iteration_\", None)\n",
    "        pred = np.clip(m.predict(val[features], num_iteration=best_iter), 0, None)\n",
    "        maes.append(mean_absolute_error(val[\"y\"], pred))\n",
    "        n_cases += 1\n",
    "    if time.time() - start > MAX_SECONDS:\n",
    "        break\n",
    "\n",
    "print(f\"Rolling MAE(mean) = {np.mean(maes):.4f} over {n_cases} folds\" if maes else \"No folds evaluated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a988c-f486-455e-bf35-36b435e4b8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a55e6b-2319-4d8c-ae64-41a82f8f7c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00a43a-04f7-4ec9-9ea7-4f870f309d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7520f6a-a266-4ce3-8b27-cc3eb0621346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5744c6-dcc4-478b-8868-c05d11586d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada0966-4533-47bb-b7d6-a05fdc0df5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786bfca-7472-4825-bdc9-acddec6e2f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631559fb-ae1b-42b3-9737-f7f906a234db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b618c9b-2c5d-4c30-a6a7-34f2f1582b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2b312-1b93-447a-9232-abb9708c04b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b875e-51c4-4705-b653-297ebf00b657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96b17c-66e6-4c22-b7b9-206e2d946d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0ff32-7f97-42e2-982c-65cdb99d7091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bcb47-2677-465e-a947-59bc2257ad2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8ec86-4770-491d-8ea2-b5c088094de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e0b71-e860-47e9-b811-375d436818f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27952ffb-3275-49c7-aff7-0fe5c1d15046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af670cdd-0575-409e-84eb-c37f85b79ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddb16c-a7dc-4dd5-bf49-b4aea7cc4b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
